{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U transformers datasets trl peft huggingface_hub"
      ],
      "metadata": {
        "id": "1lrhV17qBQt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tFaJJgyBKFw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from trl import DPOTrainer, DPOConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao0EItXgBKFx"
      },
      "source": [
        "# Preference Alignment with Direct Preference Optimization (DPO)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8CvUgROUDw-"
      },
      "source": [
        "## Step 1: Format dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCD77GZ60DOT"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "dataset = load_dataset(path=\"trl-lib/ultrafeedback_binarized\", split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHAz_3nJBKFz"
      },
      "source": [
        "## Step 2: Load Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySGlewKxBKFz"
      },
      "outputs": [],
      "source": [
        "# TODO: ü¶Å change the model to the path or repo id of the model you trained in [1_instruction_tuning](../../1_instruction_tuning/notebooks/sft_finetuning_example.ipynb)\n",
        "\n",
        "model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
        "\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# Model to fine-tune\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=model_name,\n",
        "    torch_dtype=torch.float32,\n",
        ").to(device)\n",
        "model.config.use_cache = False\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Set our name for the finetune to be saved &/ uploaded to\n",
        "finetune_name = \"SmolLM2-FT-DPO\"\n",
        "finetune_tags = [\"smol-course\", \"module_1\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeT5eUK_UJgK"
      },
      "source": [
        "## Step 3: Set up DPO Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKPILNOLR-aK"
      },
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "training_args = DPOConfig(\n",
        "    # Training batch size per GPU\n",
        "    per_device_train_batch_size=4,\n",
        "    # Number of updates steps to accumulate before performing a backward/update pass\n",
        "    # Effective batch size = per_device_train_batch_size * gradient_accumulation_steps\n",
        "    gradient_accumulation_steps=4,\n",
        "    # Saves memory by not storing activations during forward pass\n",
        "    # Instead recomputes them during backward pass\n",
        "    gradient_checkpointing=True,\n",
        "    # Base learning rate for training\n",
        "    learning_rate=5e-5,\n",
        "    # Learning rate schedule - 'cosine' gradually decreases LR following cosine curve\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    # Total number of training steps\n",
        "    max_steps=200,\n",
        "    # Disables model checkpointing during training\n",
        "    save_strategy=\"no\",\n",
        "    # How often to log training metrics\n",
        "    logging_steps=1,\n",
        "    # Directory to save model outputs\n",
        "    output_dir=\"smol_dpo_output\",\n",
        "    # Number of steps for learning rate warmup\n",
        "    warmup_steps=100,\n",
        "    # Use bfloat16 precision for faster training\n",
        "    bf16=True,\n",
        "    # Disable wandb/tensorboard logging\n",
        "    report_to=\"none\",\n",
        "    # Keep all columns in dataset even if not used\n",
        "    remove_unused_columns=False,\n",
        "    # Enable MPS (Metal Performance Shaders) for Mac devices\n",
        "    use_mps_device=device == \"mps\",\n",
        "    # Model ID for HuggingFace Hub uploads\n",
        "    hub_model_id=finetune_name,\n",
        "    # DPO-specific temperature parameter that controls the strength of the preference model\n",
        "    # Lower values (like 0.1) make the model more conservative in following preferences\n",
        "    beta=0.1,\n",
        "    # Maximum length of the input prompt in tokens\n",
        "    max_prompt_length=1024,\n",
        "    # Maximum combined length of prompt + response in tokens\n",
        "    max_length=1536,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKKe23XEBKF0"
      },
      "outputs": [],
      "source": [
        "trainer = DPOTrainer(\n",
        "    # The model to be trained\n",
        "    model=model,\n",
        "    # Training configuration from above\n",
        "    args=training_args,\n",
        "    # Dataset containing preferred/rejected response pairs\n",
        "    train_dataset=dataset,\n",
        "    # Tokenizer for processing inputs\n",
        "    processing_class=tokenizer,\n",
        "    # DPO-specific temperature parameter that controls the strength of the preference model\n",
        "    # Lower values (like 0.1) make the model more conservative in following preferences\n",
        "    # beta=0.1,\n",
        "    # Maximum length of the input prompt in tokens\n",
        "    # max_prompt_length=1024,\n",
        "    # Maximum combined length of prompt + response in tokens\n",
        "    # max_length=1536,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrveZv4xBKF1"
      },
      "source": [
        "## Step 4: Run Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G18NsokpBKF1"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "trainer.save_model(f\"./{finetune_name}\")\n",
        "\n",
        "# Save to the huggingface hub if login (HF_TOKEN is set)\n",
        "if os.getenv(\"HF_TOKEN\"):\n",
        "    trainer.push_to_hub(tags=finetune_tags)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}